{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "- No good way to introduce sample weights for GBM with sklearn wrapper\n",
    "- To add GBMs with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:24.535359Z",
     "start_time": "2019-06-15T12:04:23.357937Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "import errno\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.externals import joblib\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the relevant pieces\n",
    "- X_train, X_test, y_train, y_test\n",
    "- Feature transformer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:25.250256Z",
     "start_time": "2019-06-15T12:04:24.537178Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"../grab-ai-safety-data/X_train_transformed.pickle\")\n",
    "X_test = pd.read_pickle(\"../grab-ai-safety-data/X_test.pickle\")\n",
    "y_test = pd.read_pickle(\"../grab-ai-safety-data/y_test.pickle\")\n",
    "y_train = pd.read_pickle(\"../grab-ai-safety-data/y_train.pickle\")\n",
    "\n",
    "clean_columns = pickle.load(open(\"outputs/functions/clean_columns.pickle\", \"rb\"))\n",
    "create_windows = pickle.load(open(\"outputs/functions/create_windows.pickle\", \"rb\"))\n",
    "extract_features = pickle.load(open(\"outputs/functions/extract_features.pickle\", \"rb\"))\n",
    "min_boundary = pickle.load(open(\"outputs/functions/min_boundary.pickle\", \"rb\"))\n",
    "max_boundary = pickle.load(open(\"outputs/functions/max_boundary.pickle\", \"rb\"))\n",
    "min_speed = pickle.load(open(\"outputs/functions/min_speed.pickle\", \"rb\"))\n",
    "max_speed = pickle.load(open(\"outputs/functions/max_speed.pickle\", \"rb\"))\n",
    "min_accuracy = pickle.load(open(\"outputs/functions/min_accuracy.pickle\", \"rb\"))\n",
    "max_accuracy = pickle.load(open(\"outputs/functions/max_accuracy.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:25.557302Z",
     "start_time": "2019-06-15T12:04:25.252599Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = y_train.drop_duplicates(subset=\"bookingID\", keep=\"first\").set_index(\"bookingID\")\n",
    "y_test = y_test.drop_duplicates(subset=\"bookingID\", keep=\"first\").set_index(\"bookingID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transformations to test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.561446Z",
     "start_time": "2019-06-15T12:04:25.559117Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = (\n",
    "    X_test.pipe(\n",
    "        clean_columns\n",
    "    ).pipe(\n",
    "        create_windows\n",
    "    ).pipe(\n",
    "        extract_features\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.571952Z",
     "start_time": "2019-06-15T12:04:32.563375Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.merge(\n",
    "    X_train, \n",
    "    pd.DataFrame(y_train), \n",
    "    how=\"left\", \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    ")[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.578233Z",
     "start_time": "2019-06-15T12:04:32.573865Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test = pd.merge(\n",
    "    X_test,\n",
    "    pd.DataFrame(y_test),\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling data\n",
    "- Find a way to take care of class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.585849Z",
     "start_time": "2019-06-15T12:04:32.580004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_grouped = pd.concat([X_train, y_train], axis=1)\\ndf_majority = df_grouped[df_grouped[\"label\"] == 0]\\ndf_minority = df_grouped[df_grouped[\"label\"] == 1]\\n\\ndf_minority_upsampled = resample(\\n    df_minority,\\n    replace=True,\\n    n_samples=df_majority.count()[0],\\n    random_state=42\\n)\\n\\ndf_grouped = pd.concat(\\n    [df_majority, df_minority_upsampled]\\n)\\n\\nX_train = df_grouped.iloc[:, :-1]\\ny_train = df_grouped.iloc[:, -1]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_grouped = pd.concat([X_train, y_train], axis=1)\n",
    "df_majority = df_grouped[df_grouped[\"label\"] == 0]\n",
    "df_minority = df_grouped[df_grouped[\"label\"] == 1]\n",
    "\n",
    "df_minority_upsampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,\n",
    "    n_samples=df_majority.count()[0],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_grouped = pd.concat(\n",
    "    [df_majority, df_minority_upsampled]\n",
    ")\n",
    "\n",
    "X_train = df_grouped.iloc[:, :-1]\n",
    "y_train = df_grouped.iloc[:, -1]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipelines\n",
    "- Define pipelines\n",
    "    - GBM\n",
    "    - Logisitc Regresion\n",
    "    - Logistic Regression with PCA\n",
    "    - Random Forests\n",
    "    - Random Forests with PCA\n",
    "- Define GS grids\n",
    "- To add in grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.594144Z",
     "start_time": "2019-06-15T12:04:32.587533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipe_rf_pca = Pipeline(\\n    [\\n        (\"scl\", StandardScaler()),\\n        (\"pca\", PCA(n_components=2)),\\n        (\"clf\", RandomForestClassifier(\\n            random_state=42, \\n            class_weight=\"balanced\"))\\n    ]\\n)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = -1\n",
    "\n",
    "pipe_gbm = Pipeline(\n",
    "    [\n",
    "        (\"scl\", StandardScaler()),\n",
    "        (\"clf\", GradientBoostingClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_lr = Pipeline(\n",
    "    [\n",
    "        (\"scl\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            random_state=42, \n",
    "            penalty=\"l2\", \n",
    "            max_iter=4000)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "'''pipe_lr_pca = Pipeline(\n",
    "    [\n",
    "        (\"scl\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=2)),\n",
    "        (\"clf\", LogisticRegression( \n",
    "            random_state=42, \n",
    "            penalty=\"l2\",\n",
    "            max_iter=4000\n",
    "        ))\n",
    "    ]\n",
    ")'''\n",
    "\n",
    "pipe_rf = Pipeline(\n",
    "    [\n",
    "        (\"scl\", StandardScaler()),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            random_state=42, \n",
    "            class_weight=\"balanced\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "'''pipe_rf_pca = Pipeline(\n",
    "    [\n",
    "        (\"scl\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=2)),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            random_state=42, \n",
    "            class_weight=\"balanced\"))\n",
    "    ]\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.599641Z",
     "start_time": "2019-06-15T12:04:32.596226Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_params_gbm = [\n",
    "    {\n",
    "        \"clf__learning_rate\": [0.05, 0.1],\n",
    "        #\"clf__min_samples_split\": np.linspace(0.1, 0.5, 2),\n",
    "        #\"clf__min_samples_leaf\": np.linspace(0.1, 0.5, 2),\n",
    "        \"clf__max_depth\":[3, 5, 8],\n",
    "        \"clf__max_features\":[\"log2\"],\n",
    "        \"clf__n_estimators\":[50, 100]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_params_lr = [\n",
    "   {\n",
    "        \"clf__solver\" : [\"lbfgs\", \"saga\"],\n",
    "        \"clf__C\" : [0.1, 0.5, 1],\n",
    "   }\n",
    "]\n",
    "\n",
    "grid_params_rf = [\n",
    "    { \n",
    "        \"clf__n_estimators\" : [100],\n",
    "        \"clf__max_features\" : [0.5, 0.75],\n",
    "        \"clf__max_depth\" : [8]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:04:32.607319Z",
     "start_time": "2019-06-15T12:04:32.601759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs_rf_pca = GridSearchCV(\\n    estimator=pipe_rf_pca,\\n    param_grid=grid_params_rf,\\n    scoring=\"roc_auc\",\\n    cv=10, \\n    n_jobs=jobs\\n)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gbm = GridSearchCV(\n",
    "    estimator=pipe_gbm,\n",
    "    param_grid=grid_params_gbm,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10,\n",
    "    n_jobs=jobs\n",
    ")\n",
    "\n",
    "gs_lr = GridSearchCV(\n",
    "    estimator=pipe_lr,\n",
    "    param_grid=grid_params_lr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10\n",
    ") \n",
    "\n",
    "'''gs_lr_pca = GridSearchCV(\n",
    "    estimator=pipe_lr_pca,\n",
    "    param_grid=grid_params_lr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10\n",
    ")'''\n",
    "\n",
    "gs_rf = GridSearchCV(\n",
    "    estimator=pipe_rf,\n",
    "    param_grid=grid_params_rf,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10, \n",
    "    n_jobs=jobs\n",
    ")\n",
    "\n",
    "'''gs_rf_pca = GridSearchCV(\n",
    "    estimator=pipe_rf_pca,\n",
    "    param_grid=grid_params_rf,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10, \n",
    "    n_jobs=jobs\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:47.552638Z",
     "start_time": "2019-06-15T12:04:32.609244Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Gradient Boosted Machine\n",
      "Best params: {'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__max_features': 'log2', 'clf__n_estimators': 100}\n",
      "Best training accuracy: 0.724\n",
      "Test set accuracy score for best params: 0.781 \n",
      "ROC_AUC score for best params: 0.727\n",
      "\n",
      "Estimator: Logistic Regression\n",
      "Best params: {'clf__C': 1, 'clf__solver': 'lbfgs'}\n",
      "Best training accuracy: 0.717\n",
      "Test set accuracy score for best params: 0.781 \n",
      "ROC_AUC score for best params: 0.723\n",
      "\n",
      "Estimator: Random Forest\n",
      "Best params: {'clf__max_depth': 8, 'clf__max_features': 0.75, 'clf__n_estimators': 100}\n",
      "Best training accuracy: 0.724\n",
      "Test set accuracy score for best params: 0.700 \n",
      "ROC_AUC score for best params: 0.723\n",
      "\n",
      "Classifier with best test set accuracy: Gradient Boosted Machine\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './outputs/models/41786/Gradient Boosted Machine_0.73.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-236b7e301ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m joblib.dump(\n\u001b[1;32m     55\u001b[0m     \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./outputs/models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime_stamp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m ) \n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/models/41786/Gradient Boosted Machine_0.73.pkl'"
     ]
    }
   ],
   "source": [
    "grids = [\n",
    "    gs_gbm, \n",
    "    gs_lr, \n",
    "    # gs_lr_pca, \n",
    "    gs_rf, \n",
    "    # gs_rf_pca\n",
    "]\n",
    "\n",
    "grid_dict = {\n",
    "    0: \"Gradient Boosted Machine\",\n",
    "    1: \"Logistic Regression\", \n",
    "    # 2: \"Logistic Regression w/PCA\", \n",
    "    2: \"Random Forest\", \n",
    "    # 4: \"Random Forest w/PCA\", \n",
    "}\n",
    "\n",
    "# Fit the grid search objects\n",
    "print(\"Performing model optimizations...\")\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "time_stamp = []\n",
    "for idx, gs in enumerate(grids):\n",
    "    print(\"\\nEstimator: %s\" % grid_dict[idx])\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params: %s\" % gs.best_params_)\n",
    "    print(\"Best training accuracy: %.3f\" % gs.best_score_)\n",
    "\n",
    "    y_pred = gs.predict(X_test)\n",
    "    y_prob = gs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"Test set accuracy score for best params: %.3f \" % accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC_AUC score for best params: %.3f\" % roc_auc_score(y_test, y_prob))\n",
    "\n",
    "    if roc_auc_score(y_test, y_prob) > best_acc:\n",
    "        best_acc = roc_auc_score(y_test, y_prob)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "        time_stamp = str(datetime.now(tz=None).microsecond)\n",
    "print(\"\\nClassifier with best test set accuracy: {}\".format(grid_dict[best_clf]))\n",
    "\n",
    "# Dumps the corresponding best model and functions into a specific folder\n",
    "\n",
    "model_destination = \"./outputs/models\" + \"/\" + time_stamp\n",
    "if not os.path.exists(model_destination):\n",
    "    try:\n",
    "        os.makedirs(model_destination)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "joblib.dump(\n",
    "    gs, \"./outputs/models/\" + time_stamp + \"/\" + grid_dict[best_clf] + \n",
    "    \"_\" + str(np.round(best_acc, 2)) + \".pkl\"\n",
    ") \n",
    "\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"clean_columns.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(clean_columns, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"create_windows.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(create_windows, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"extract_features.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(extract_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"min_boundary.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(min_boundary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"max_boundary.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(max_boundary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"min_accuracy.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(min_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"max_accuracy.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(max_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"min_speed.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(min_speed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"max_speed.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(max_speed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\n",
    "    \"{} and associated functions have been successfully exported\".format(grid_dict[best_clf])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:14:41.640977Z",
     "start_time": "2019-06-15T12:14:41.638489Z"
    }
   },
   "outputs": [],
   "source": [
    "time_stamp = '420932'\n",
    "model_destination = \"./outputs/models\" + \"/\" + time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:14:43.010313Z",
     "start_time": "2019-06-15T12:14:43.006746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/models/420932'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:14:47.080695Z",
     "start_time": "2019-06-15T12:14:47.077184Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(model_destination):\n",
    "    try:\n",
    "        os.makedirs(model_destination)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:14:48.362146Z",
     "start_time": "2019-06-15T12:14:48.357033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/models/420932'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(\n",
    "    gs, \"./outputs/models/\" + time_stamp + \"/\" + grid_dict[best_clf] + \n",
    "    \"_\" + str(np.round(best_acc, 2)) + \".pkl\"\n",
    ") \n",
    "\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"clean_columns.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(clean_columns, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"create_windows.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(create_windows, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"extract_features.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(extract_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"min_boundary.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(min_boundary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"max_boundary.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(max_boundary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"min_accuracy.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(min_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"max_accuracy.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(max_accuracy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"min_speed.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(min_speed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"outputs/models/\" + time_stamp + \"/\" + \"max_speed.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(max_speed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:47.553534Z",
     "start_time": "2019-06-15T12:04:23.360Z"
    }
   },
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    0: \"blue\",\n",
    "    1: \"red\", \n",
    "    2: \"green\", \n",
    "    3: \"black\", \n",
    "    4: \"purple\", \n",
    "}\n",
    "plt.figure(figsize=(20, 15))\n",
    "for idx, gs in enumerate(grids):\n",
    "    probs = gs.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title(\"Receiver Operating Characteristic\")\n",
    "    plt.plot(\n",
    "        fpr, \n",
    "        tpr, \n",
    "        color=color_dict[idx], \n",
    "        label = \"%s AUC = %0.2f\" % (grid_dict[idx], roc_auc)\n",
    "    )\n",
    "    plt.legend(loc = \"lower right\")\n",
    "    plt.plot([0, 1], [0, 1], \"r--\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:47.554520Z",
     "start_time": "2019-06-15T12:04:23.361Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    random_state=42, \n",
    "    class_weight=\"balanced\"\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "feature_importances = pd.DataFrame(\n",
    "    rf.feature_importances_,\n",
    "    index = X_train.columns,\n",
    "    columns=[\"importance\"]\n",
    ").sort_values(\n",
    "    \"importance\", \n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:47.555513Z",
     "start_time": "2019-06-15T12:04:23.363Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(feature_importances.index, feature_importances[\"importance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "- Check if there is data leakage?\n",
    "- A sudden jump to 90% accuracy seems dubious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:47.556575Z",
     "start_time": "2019-06-15T12:04:23.364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "cross_val_score(\n",
    "    pipe_rf, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=cv\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
